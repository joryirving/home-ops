---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema_json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: comfyui
spec:
  chartRef:
    kind: OCIRepository
    name: app-template
  dependsOn: []
  interval: 15m
  values:
    controllers:
      comfyui:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: ghcr.io/simonlui/docker_ipex_comfyui
              tag: latest
            env:
              # Intel GPU specific environment variables based on the image's Dockerfile
              ONEAPI_DEVICE_SELECTOR: "level_zero:gpu"  # Use Intel GPU via Level Zero
              ZES_ENABLE_SYSMAN: "1"  # Enable Level Zero system management
              NEOReadDebugKeys: "1"   # Enable debug keys for Intel GPU
              ClDeviceGlobalMemSizeAvailablePercent: "100"  # Force 100% available VRAM
              SYCL_CACHE_PERSISTENT: "1"  # Enable SYCL persistent cache
              SYCL_PI_LEVEL_ZERO_SINGLE_THREAD_MODE: "1"  # Enable SYCL single thread mode
              # ComfyUI arguments to run in the container
              ComfyArgs: "--listen --port 8188 --auto-launch"
              # Set Python path and library path as needed by the Intel oneAPI components
              PYTHONPATH: "/opt/intel/oneapi/python/python-libs"
              LD_LIBRARY_PATH: "/opt/intel/oneapi/lib:/opt/intel/oneapi/compiler/latest/linux/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib64"
            resources:
              claims:
                - name: gpu
              requests:
                cpu: 2000m
                memory: 8Gi
              limits:
                cpu: 4000m
                memory: 16Gi
    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 100
        fsGroup: 100
        fsGroupChangePolicy: "OnRootMismatch"
      resourceClaims:
        - name: gpu
          resourceClaimTemplateName: "${APP}-gpu"
    service:
      app:
        ports:
          http:
            port: 8188
    route:
      app:
        hostnames: ["comfyui.jory.dev"]
        parentRefs:
          - name: envoy-internal
            namespace: network
    persistence:
      models:
        enabled: true
        existingClaim: "{{ .Release.Name }}-models"
        globalMounts:
          - path: /ComfyUI/models
            readOnly: false
      outputs:
        enabled: true
        existingClaim: "{{ .Release.Name }}-outputs"
        globalMounts:
          - path: /ComfyUI/output
      deps:
        enabled: true
        existingClaim: "{{ .Release.Name }}-deps"
        globalMounts:
          - path: /deps2
      temp:
        enabled: true
        type: emptyDir
        globalMounts:
          - path: /tmp
