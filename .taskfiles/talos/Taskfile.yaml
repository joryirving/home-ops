---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

tasks:
  genconfig:
    desc: Generate the Talos configs
    dir: '{{.TALOS_DIR}}'
    cmd: talhelper genconfig  --config-file talconfig.yaml --secret-file talsecret.sops.yaml --out-dir clusterconfig
    requires:
      vars: [CLUSTER]
    preconditions:
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - test -f {{.TALOS_DIR}}/talsecret.sops.yaml

  apply-node:
    desc: Apply Talos config to a node [CLUSTER=main] [HOSTNAME=required] [MODE=auto]
    dir: '{{.TALOS_DIR}}'
    cmds:
      # - task: down
      - talosctl apply-config --nodes {{.HOSTNAME}} --mode={{.MODE}} --file clusterconfig/{{.CLUSTER}}-{{.HOSTNAME}}.yaml --talosconfig clusterconfig/talosconfig
      - talosctl --nodes {{.HOSTNAME}} health --talosconfig clusterconfig/talosconfig
      # - task: up
    vars:
      MODE: '{{.MODE | default "auto"}}'
    requires:
      vars: [CLUSTER, HOSTNAME]
    preconditions:
      - talosctl --nodes {{.HOSTNAME}} get machineconfig --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - test -f {{.TALOS_DIR}}/cluster.env
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - test -f {{.TALOS_DIR}}/clusterconfig/talosconfig

  apply-cluster: ## This isn't working on utility cluster
    desc: Apply the Talos config on all nodes for an existing cluster [CLUSTER=main]
    vars:
      HOSTNAMES:
        sh: kubectl get nodes --output=jsonpath='{.items[*].metadata.name}' --context {{.CLUSTER}}
    cmds:
      - for: { var: HOSTNAMES }
        task:  apply-node
        vars:
          HOSTNAME: '{{.ITEM}}'
          CLUSTER: "{{.CLUSTER}}"
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl config info --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - test -f {{.TALOS_DIR}}/cluster.env
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - test -f {{.TALOS_DIR}}/clusterconfig/talosconfig

  soft-nuke:
    desc: Resets nodes back to maintenance mode so you can re-deploy again straight after
    prompt: This will destroy your cluster and reset the nodes back to maintenance mode... continue?
    dir: '{{.TALOS_DIR}}'
    cmd: talhelper gencommand reset --out-dir clusterconfig --config-file talconfig.yaml --extra-flags "--reboot --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --graceful=false --wait=false" | bash
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }

  hard-nuke:
    desc: Resets nodes back completely and reboots them
    prompt: This will destroy your cluster and reset the nodes... continue?
    dir: '{{.TALOS_DIR}}'
    cmd: talhelper gencommand reset --out-dir clusterconfig --config-file talconfig.yaml --extra-flags "--reboot --graceful=false --wait=false" | bash
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }

  reboot-node:
    desc: Reboot Talos on a single node [CLUSTER=main] [HOSTNAME=required]
    cmds:
      - task: down
      - talosctl --nodes {{.HOSTNAME}} reboot --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - talosctl --nodes {{.HOSTNAME}} health --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - task: up
    requires:
      vars: [CLUSTER, HOSTNAME]
    preconditions:
      - talosctl --nodes {{.HOSTNAME}} get machineconfig --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - talosctl config info --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - test -f {{.TALOS_DIR}}/cluster.env
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - which talosctl

  reboot-cluster:
    desc: Reboot Talos across the whole cluster [CLUSTER=main]
    prompt: This will reboot all of the cluster nodes. Are you sure you want to continue?
    requires:
      vars: [CLUSTER]
    vars:
      HOSTNAMES:
        sh: kubectl get nodes --output=jsonpath='{.items[*].metadata.name}' --context {{.CLUSTER}}
    cmds:
      - for: { var: HOSTNAMES }
        task: reboot-node
        vars:
          HOSTNAME: '{{.ITEM}}'
          CLUSTER: "{{.CLUSTER}}"
      - task: :kubernetes:delete-failed-pods
        vars:
          CLUSTER: "{{.CLUSTER}}"
    preconditions:
      - talosctl config info --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - test -f {{.TALOS_DIR}}/cluster.env
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - test -f {{.TALOS_DIR}}/clusterconfig/talosconfig

  shutdown-cluster:
    desc: Shutdown Talos across the whole cluster [CLUSTER=main]
    prompt: Shutdown the Talos cluster '{{.CLUSTER}}' ... continue?
    cmd: talosctl shutdown --nodes {{.HOSTNAMES}} --force
    vars:
      HOSTNAMES:
        sh: kubectl get nodes --output=jsonpath='{.items[*].metadata.name}'
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl --nodes {{.NODES}} get machineconfig --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - talosctl config info --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - test -f {{.TALOS_DIR}}/clusterconfig/talosconfig
      - which talosctl

  kubeconfig:
    desc: Generate the kubeconfig for a Talos cluster [CLUSTER=main]
    cmd: talosctl kubeconfig --nodes {{.TALOS_CONTROLLER}} --force --force-context-name {{.CLUSTER}} {{.CLUSTER_DIR}}
    vars:
      TALOS_CONTROLLER:
        sh: talosctl --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig config info --output json --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig | jq --raw-output '.endpoints[]' | shuf -n 1
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl config info --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - test -f {{.TALOS_DIR}}/clusterconfig/talosconfig
      - which talosctl

  down:
    internal: true
    cmds:
      - '{{if eq .CLUSTER "main"}}until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done{{end}}'
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
      - task: :volsync:state-suspend
    preconditions:
      - which kubectl

  up:
    internal: true
    cmds:
      - '{{if eq .CLUSTER "main"}}until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done{{end}}'
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
      - task: :volsync:state-resume
    preconditions:
      - which kubectl
